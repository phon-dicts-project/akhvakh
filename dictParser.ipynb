{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286895"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'wholeDictionaryFromDoc.htm'\n",
    "\n",
    "with open(fname, 'r') as f:\n",
    "    global_soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    to_soup = str(global_soup.find('div', class_ = 'WordSection2')) + str(global_soup.find('div', class_ = 'WordSection3'))\n",
    "    global_soup = BeautifulSoup(to_soup, 'html.parser')\n",
    "paragraphs = global_soup.find_all('p')\n",
    "\n",
    "open('wholeDictionaryFromDoc_dictOnly.htm', 'w', encoding='utf-8').write(str(global_soup))\n",
    "open('plainDictText_div23.txt', 'w', encoding='utf-8').write(global_soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawParagraphsList = [re.sub('\\n', ' ', str(p)) for p in paragraphs]\n",
    "rawParagraphsStr = '\\n'.join(rawParagraphsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsStr.htm', 'w', encoding='utf-8') as f:\n",
    "    f.write(rawParagraphsStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## произошел ручной эдитинг пары абзацев, которые плохо соединились ввиду\n",
    "## 1) плохого распознания исходного документа\n",
    "## 2) соединения двух <div> в один\n",
    "\n",
    "## вручную втч. были удалены титульные абзацы (было быстрее, чем текстом)\n",
    "\n",
    "## 'rawParagraphsStr.htm' = основной рабочий файл теперь, там решена проблема начальных ЛЪ_ \n",
    "## сперва надо будет чистить от тэгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsStr.htm', 'r', encoding='utf-8') as f:\n",
    "    plainDict = f.read()\n",
    "    plainDict = re.sub('</?o:p>', '', plainDict)\n",
    "    plainDict = re.sub('</?s?pa?n?[^>]*>', '', plainDict)\n",
    "    \n",
    "    plainDict = re.sub('</?i[^>]*>', '_', plainDict)\n",
    "    plainDict = re.sub('</?b[^>]*>', '*', plainDict)\n",
    "    \n",
    "    \n",
    "    ## назализация: потом надо будет заменить на надстрочный \"н\"\n",
    "    plainDict = re.sub('<sup>[нН].?</sup>', 'ᴴ', plainDict)\n",
    "    \n",
    "    plainDict = re.sub('\\*\\*', '', plainDict)\n",
    "    plainDict = re.sub('__', '', plainDict)\n",
    "    \n",
    "    plainDict = re.sub('_\\.', '._', plainDict)\n",
    "    \n",
    "    with open('rawParagraphsCleared.htm', 'w', encoding='utf-8') as fw:\n",
    "        fw.write(plainDict)\n",
    "    #plainDict = re.sub('<sup>Н</sup>')\n",
    "    \n",
    "    # надо подумать о неразрывных пробелах и о лишних пробелах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['Trifolium repens', 'Condoliulus arvense', 'ARCHANGELICA OFFICINALIS', 'Caloglossum ofride', 'Primula verisl ', 'Salvia', 'Anetrum lapg', 'Menthac longlfaem', 'Fumoria Sehleicheri ', 'Asperula hamufusa', 'stachy palustris', 'Astemia axsinthum', '*хамесциациум* ', 'Peucejanum nuthenicum ', 'Seseli franscaucastce ', 'lapsano media', 'Pactulus glomepafa', 'Stellarta media', 'Carum carvi', 'Vitia', 'Thumus seppillum l ', 'Ziziphora Sp ', 'Hyoscyamus niger', 'Plantago major', 'Galium aparine', 'Zathyrus minuatus', 'Chenopodium Sp', 'Vonatcum labelianum', 'Pragnuges Communis', 'Anthriyscus nemoralis', 'Cenfiana cructafa', 'chocnophyclum', 'Origanum vulgare', 'Cubus saxatitis', 'loricera xylosfeum', 'Rapistrum rugogum ', 'Galega arientalis', 'Rumex acelosella', 'inonotus obbiglus ', 'Malka neglecta']\n"
     ]
    }
   ],
   "source": [
    "with open('rawParagraphsCleared.htm', 'r', encoding='utf-8') as f:\n",
    "    plainDict = f.read()\n",
    "    \n",
    "regLatin = re.compile('_лат\\._ ([^\\(\\n;\\d_]+)')\n",
    "latins = regLatin.findall(plainDict)\n",
    "print(len(latins))\n",
    "print(latins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listOfLatinNamesInDict.txt', 'w', encoding='utf-8') as fl:\n",
    "    fl.write('\\n'.join(latins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainDict = re.sub(' ', ' ', plainDict)\n",
    "plainDict = re.sub('\\r', '', plainDict)\n",
    "plainDict = re.sub('\\n ', '\\n', plainDict)\n",
    "plainDict = re.sub('\\n\\* ', '\\n*', plainDict)\n",
    "plainDict = re.sub('<sup> </sup>', ' ', plainDict)\n",
    "plainDict = re.sub(r'<sup>(\\d)</sup>', r'№\\1', plainDict)\n",
    "plainDict = re.sub('\\*\\*', '', plainDict)\n",
    "\n",
    "with open('rawParagraphsCleared_v2.txt', 'w', encoding='utf-8') as fv2:\n",
    "    fv2.write(plainDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <sup>н</sup> = \n",
    "## <sup>Н</sup> = \n",
    "## </?i> = _\n",
    "## </?b> = *\n",
    "## <sup>.</sup> = \n",
    "\n",
    "## ̄надстрочная палка; символ стоит после последнего символа лигатуры, но перед ударением:\n",
    "#### ЛЪ̄\n",
    "#### а̄'\n",
    "\n",
    "\n",
    "## symbols_pivot\n",
    "## E:\n",
    "### 1. E(\\s) = Е \n",
    "### 2. E(\\S) = Л̄\n",
    "\n",
    "## swap(_I, I_)\n",
    "\n",
    "## \"(\\d\\))([а-яёА-ЯЁa-zA-Z])\", r\"\\1 \\2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsCleared_v2.txt', 'r', encoding='utf-8') as fv2:\n",
    "    dictPlain = fv2.read()\n",
    "\n",
    "with open('symbols_pivot.csv', 'r', encoding='utf-8') as ft:\n",
    "    rawTable = re.sub('\\r', '', ft.read())\n",
    "    table = [x.split(';') for x in rawTable.split('\\n') if len(x.split(';')) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in table:\n",
    "    dictPlain = re.sub(pair[0], pair[1], dictPlain)\n",
    "    \n",
    "dictPlain = re.sub('̄I', 'Ī', dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsClearedAndReplaced.txt', 'w', encoding='utf-8') as fv3:\n",
    "    fv3.write(dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsClearedAndReplaced.txt', 'r', encoding='utf-8') as fv3:\n",
    "    dictPlain = fv3.read()\n",
    "    testSymbols = re.sub('_лат\\._ \\S+ \\S+', '', dictPlain)\n",
    "    checkset = set(re.findall('[^а-яА-ЯI_\\*,\\.№\\) \\n]', testSymbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вручную заменил все инвалидные (то есть побитые при операциях замены) тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'\",\n",
       " '(',\n",
       " '-',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '\\\\',\n",
       " 'e',\n",
       " 'g',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'o',\n",
       " 's',\n",
       " 'u',\n",
       " 'x',\n",
       " 'y',\n",
       " '«',\n",
       " '»',\n",
       " '̄',\n",
       " 'ё',\n",
       " 'ᴴ',\n",
       " '–',\n",
       " '…',\n",
       " '◊'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenLatin = regLatin.findall(dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Ц̄ц̄ifolium ц̄eц̄eи's\",\n",
       " \"К̄oи'doliulus л̄ц̄veи'se\",\n",
       " \"С̄И'К̄лъ̄С̄Х̄GЛ̄цIĪК̄С̄ х̄л̄лĪКĪХ̄С̄цIĪи'\",\n",
       " 'К̄л̄loglossum ofц̄ide',\n",
       " 'Pц̄imulл̄ veц̄isl ',\n",
       " \"и'л̄lviл̄\",\n",
       " \"С̄и'eӣц̄um lл̄ц̄g\",\n",
       " \"ЦĪeи'ӣhл̄лъ̄ loи'glfл̄em\",\n",
       " \"л̄umoц̄iл̄ и'ehleiлъ̄heц̄i \",\n",
       " 'С̄sц̄eц̄ulл̄ hл̄mufusл̄',\n",
       " 'sӣл̄лъ̄hy ц̄л̄lusӣц̄is',\n",
       " \"С̄sӣemiл̄ л̄xsiи'ӣhum\",\n",
       " '*хамесциациум* ',\n",
       " \"Peuлъ̄eх̄л̄и'um и'uӣheи'iлъ̄um \",\n",
       " \"и'eseli fц̄л̄и'sлъ̄л̄uлъ̄л̄sӣлъ̄e \",\n",
       " \"lл̄ц̄sл̄и'o mediл̄\",\n",
       " 'Pл̄лъ̄ӣulus glomeц̄л̄fл̄',\n",
       " \"и'ӣellл̄ц̄ӣл̄ mediл̄\",\n",
       " 'К̄л̄ц̄um лъ̄л̄ц̄vi',\n",
       " 'Ч̄iӣiл̄',\n",
       " 'Ц̄humus seц̄ц̄illum l ',\n",
       " \"Ӯiziц̄hoц̄л̄ и'ц̄ \",\n",
       " \"лъ̄yosлъ̄yл̄mus и'igeц̄\",\n",
       " \"Plл̄и'ӣл̄go mл̄х̄oц̄\",\n",
       " \"Gл̄lium л̄ц̄л̄ц̄iи'e\",\n",
       " \"Ӯл̄ӣhyц̄us miи'uл̄ӣus\",\n",
       " \"К̄heи'oц̄odium и'ц̄\",\n",
       " \"Ч̄oи'л̄ӣлъ̄um lл̄beliл̄и'um\",\n",
       " \"Pц̄л̄gи'uges К̄ommuи'is\",\n",
       " \"С̄и'ӣhц̄iysлъ̄us и'emoц̄л̄lis\",\n",
       " \"К̄eи'fiл̄и'л̄ лъ̄ц̄uлъ̄ӣл̄fл̄\",\n",
       " \"лъ̄hoлъ̄и'oц̄hyлъ̄lum\",\n",
       " \"х̄ц̄igл̄и'um vulgл̄ц̄e\",\n",
       " 'К̄ubus sл̄xл̄ӣiӣis',\n",
       " 'loц̄iлъ̄eц̄л̄ xylosfeum',\n",
       " \"И'л̄ц̄isӣц̄um ц̄ugogum \",\n",
       " \"Gл̄legл̄ л̄ц̄ieи'ӣл̄lis\",\n",
       " \"И'umex л̄лъ̄elosellл̄\",\n",
       " \"iи'oи'oӣus obbiglus \",\n",
       " \"ЦĪл̄lkл̄ и'egleлъ̄ӣл̄\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brokenLatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# точно ли \"сломанным\" латинским названиям соответствует лишь один фрагмент всего документа\n",
    "def isItSafeToReSubLatins(brokenLatin=brokenLatin, plainDict=plainDict):\n",
    "    for bl in brokenLatin:\n",
    "        bl = re.sub('\\*', '\\\\\\*', bl)\n",
    "        if len(re.findall(bl, plainDict)) > 1:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isItSafeToReSubLatins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listOfLatinNamesInDict.txt', 'r', encoding='utf-8') as flt:\n",
    "    trueLatNames = re.sub('\\r', '', flt.read()).split('\\n')\n",
    "    for index, item in enumerate(trueLatNames):\n",
    "        bl = brokenLatin[index]\n",
    "        bl = re.sub('\\*', '\\\\\\*', bl)\n",
    "        dictPlain = re.sub(bl, item, dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsClearedAndReplacedFinal.txt', 'w', encoding='utf-8') as fv3:\n",
    "    fv3.write(dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsClearedAndReplacedFinal.txt', 'r', encoding='utf-8') as fv3:\n",
    "    dictPlain = fv3.read()\n",
    "    \n",
    "dictPlain = re.sub('\\(-\\*', '(*-', dictPlain)\n",
    "dictPlain = re.sub(\"'ᴴ\", \"ᴴ'\", dictPlain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вручную добавил разметку на главы П и С (?), там почему-то слетели b-тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rawParagraphsClearedAndReplacedFinal.txt', 'r', encoding='utf-8') as fv3:\n",
    "    dictPlain = fv3.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = dictPlain.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7132"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['КĪВ', 'kʼʷː'], ['КЪIВ', 'q͡χ’ʷ'], ['КЬIВ', 't͡ɬʼʷ'], ['ЛЪ̄В', 'ɬʷː'], ['ЛЪIВ', 't͡ɬʷ'], ['ХЪIВ', 'q͡χʷ'], ['ЦĪВ', 't͡s’ʷː'], ['ЧĪВ', 't͡ʃ’ʷː'], ['Аᴴ̄', 'ãː'], ['ГЬВ', 'hʷ'], ['ГЪВ', 'ʁʷ'], ['ГIВ', 'ʕʷ'], ['Еᴴ̄', 'ẽː'], ['ДЖВ', 'd͡ʒʷ'], ['Иᴴ̄', 'ĩː'], ['К̄В', 'kʷː'], ['КЪВ', 'q͡χ’ʷː'], ['КЬВ', 't͡ɬʼʷː'], ['КIВ', 'k’ʷ'], ['КĪ', 'kʼː'], ['КЪI', 'q͡χ’'], ['КЬI', 't͡ɬʼ'], ['ЛЪ̄', 'ɬː'], ['ЛЪI', 't͡ɬ'], ['ЛЪВ', 'ɬʷ'], ['ЛIВ', 't͡ɬʷː'], ['Оᴴ̄', 'õ'], ['С̄В', 'sʷː'], ['ТIВ', 't’ʷ'], ['Уᴴ̄', 'ũː'], ['Х̄В', 'χʷː'], ['ХЪВ', 'q͡χʷː'], ['ХЪI', 'q͡χ'], ['ХЬВ', 'xʷː'], ['ХIВ', 'ћʷ'], ['Ц̄В', 't͡sʷː'], ['ЦĪ', 't͡s’ː'], ['Ч̄В', 't͡ʃ’ʷː'], ['ЧIВ', 't͡ʃ’ʷ'], ['ЧĪ', 't͡ʃ’ː'], ['Эᴴ̄', 'ẽː'], ['Аᴴ', 'ã'], ['А̄', 'aː'], ['ГВ', 'gʷ'], ['ГЬ', 'h'], ['ГЪ', 'ʁ'], ['ГI', 'ʕ'], ['ДВ', 'dʷ'], ['Еᴴ', 'ẽ'], ['Е̄', 'eː'], ['ЖВ', 'ʒʷ'], ['ДЖ', 'd͡ʒ'], ['ЗВ', 'zʷ'], ['Иᴴ', 'ĩ'], ['Ӣ', 'iː'], ['КВ', 'kʷ'], ['К̄', 'kː'], ['КЪ', 'q͡χ’ː'], ['КЬ', 't͡ɬʼː'], ['КI', 'k’'], ['ЛЪ', 'ɬ'], ['Л̄', 'lː'], ['ЛI', 't͡ɬː'], ['ЛВ', 'lʷ'], ['М̄', 'mː'], ['О̄', 'oː'], ['Оᴴ', 'õː'], ['ПI', 'p’'], ['СВ', 'sʷ'], ['С̄', 'sː'], ['ТВ', 'tʷ'], ['ТI', 't’'], ['Уᴴ', 'ũ'], ['Ӯ', 'uː'], ['ХВ', 'χʷ'], ['Х̄', 'χː'], ['ХЪ', 'q͡χː'], ['ХЬ', 'xː'], ['ХI', 'ћ'], ['Ц̄', 't͡sː'], ['ЦI', 't͡s’'], ['ЧВ', 't͡ʃʷ'], ['Ч̄', 't͡ʃː'], ['ЧI', 't͡ʃ’'], ['ШВ', 'ʃʷ'], ['ЩВ', 'ʃʷː'], ['ЪВ', 'ʔʷ'], ['Эᴴ', 'ẽ'], ['Э̄', 'eː'], ['А', 'a'], ['Б', 'b'], ['В', 'w'], ['Г', 'g'], ['Д', 'd'], ['Е', 'e'], ['Ж', 'ʒ'], ['З', 'z'], ['И', 'i'], ['Й', 'j'], ['К', 'k'], ['Л', 'l'], ['М', 'm'], ['Н', 'n'], ['О', 'o'], ['П', 'p'], ['Р', 'r'], ['С', 's'], ['Т', 't'], ['У', 'u'], ['Х', 'χ'], ['Ц', 't͡s'], ['Ч', 't͡ʃ'], ['Ш', 'ʃ'], ['Щ', 'ʃː'], ['Ъ', 'ʔ'], ['Э', 'e']]\n"
     ]
    }
   ],
   "source": [
    "# импорт составленной таблицы букво-фонемных соответствий\n",
    "with open('ipaChart.txt', 'r', encoding='utf-8') as ipaF:\n",
    "    rawTable = re.sub('\\r', '', ipaF.read())\n",
    "    ipaTable = [x.split(';') for x in rawTable.split('\\n') if len(x.split(';')) == 2]\n",
    "    ipaTable = sorted(ipaTable, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "print(ipaTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonetizerPlus(word, ipaTable=ipaTable, removeSlash=True):\n",
    "    #print(word)\n",
    "    word = re.sub('- ?', '', word.strip())\n",
    "    word = re.sub('\\([^\\)][\\)$]', '', word)\n",
    "    #classMarker = re.findall('.?/[^/]{1/.?',\n",
    "    phonWord = word\n",
    "    for pair in ipaTable:\n",
    "        cyr = pair[0]\n",
    "        repl = '-' + pair[1] + '-'\n",
    "        phonWord = re.sub(cyr, repl, phonWord)\n",
    "        #print(cyr, repl, phonWord)\n",
    "    #print(phonWord)\n",
    "    phonWord = re.sub('--+', '-', phonWord)\n",
    "    phonWord = re.sub(\"-'-\", \"'-\", phonWord)\n",
    "    phonWord = re.sub(\"- \", ' ', phonWord)\n",
    "    phonWord = re.sub(\" -\", ' ', phonWord)\n",
    "    if removeSlash:\n",
    "        phonWord = re.sub(\"-/-\", '-', phonWord)\n",
    "        phonWord = re.sub(\"-'/\", \"'-\", phonWord)\n",
    "    return re.sub('i', 'I', word.lower()), re.sub('--+', '-', phonWord).strip('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "globalList = list()\n",
    "for rawArticle in paragraphs:\n",
    "    \n",
    "    rawArticle = re.sub('№(\\d)', '', rawArticle)\n",
    "    #print(rawArticle)\n",
    "    try:\n",
    "        boldSpans = re.findall('\\*([^\\*]+)\\*', rawArticle)\n",
    "        rawLemmaFirst = boldSpans[0]\n",
    "        if len(boldSpans) > 1:\n",
    "            if len(re.findall('[А-Я]', boldSpans[1])) > 2:\n",
    "                rawLemmaFirst = ''.join(rawArticle.split('*', maxsplit=4)[:4])\n",
    "                offset = 4\n",
    "            else:\n",
    "                offset = 2\n",
    "        else:\n",
    "            offset = 2\n",
    "    except:\n",
    "        print(rawArticle)\n",
    "    #print(rawLemmaFirst)\n",
    "    \n",
    "    \n",
    "     \n",
    "    if re.search(':', rawLemmaFirst):\n",
    "        rawLemmaFirst = rawLemmaFirst.split(':')[0]\n",
    "    \n",
    "    rawLemma = re.sub(' I+', '', rawLemmaFirst)\n",
    "    \n",
    "    #rawLemmas = [x.strip() for x in rawLemma.split('//')]\n",
    "    \n",
    "    \n",
    "    simpleGrammar = re.search('\\([^\\)]+\\)', rawLemmaFirst)\n",
    "    if simpleGrammar:\n",
    "        morphology = simpleGrammar.group()\n",
    "        rawLemma = re.split(morphology, rawLemma)[0]\n",
    "        rawLemmaFirst = rawLemma\n",
    "        if '_' in morphology:\n",
    "            morphology = re.sub('_[^_]+_', '', morphology)\n",
    "            if len(morphology) < 5:\n",
    "                morphology = ''\n",
    "                definition = rawArticle[len(rawLemmaFirst)+offset:]\n",
    "        \n",
    "    else:    \n",
    "        rawGrammar = re.search('\\(([^\\)]*\\*[^\\)]+)\\)', rawArticle)\n",
    "    \n",
    "        if rawGrammar:\n",
    "            rawMorphology = rawGrammar.group()\n",
    "            morphology = re.sub('_[^_]+_', '', rawMorphology)\n",
    "            if len(morphology) < 5:\n",
    "                morphology = ''\n",
    "                definition = rawArticle[len(rawLemmaFirst)+offset:]\n",
    "            else:\n",
    "                morphology = re.sub('\\*', '', morphology) \n",
    "                morphology = '(' + morphology.strip(')(').strip(',') + ')'\n",
    "                definition = rawArticle.split(rawMorphology)[1].strip()\n",
    "        #print(1, morphology)\n",
    "        else:\n",
    "        \n",
    "            morphology = ''\n",
    "            definition = rawArticle[len(rawLemmaFirst)+offset:]\n",
    "        \n",
    "    rawLemmas = [x.strip() for x in re.split(' ?[ /][ /] ?', rawLemma)]\n",
    "    for rawLemma in rawLemmas:\n",
    "        #print(morphology)\n",
    "        if len(rawLemma) >= 2:\n",
    "            articleDict = dict()\n",
    "            lemma_source, lemma_ipa = phonetizerPlus(rawLemma)\n",
    "            articleDict['morphology'] = morphology\n",
    "            articleDict['lemma_source'] = lemma_source\n",
    "            articleDict['lemma_ipa'] = lemma_ipa\n",
    "            articleDict['definition'] = definition\n",
    "            globalList.append(articleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8003"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('akhvakhDBv3.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    csvfile.write('word_id' + '\\t' + '\\t'.join((list(articleDict.keys()))))\n",
    "    for index, article in enumerate(globalList, start=1):\n",
    "        csvfile.write('\\n')\n",
    "        row = [str(index)]\n",
    "        for key in article:\n",
    "            row.append(article[key])\n",
    "        rowS = '\\t'.join(row)\n",
    "        csvfile.write(rowS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
